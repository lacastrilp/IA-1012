import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression

# ===============================
# ğŸ“Œ FunciÃ³n para cargar dataset
# ===============================
st.title("ğŸ” ClasificaciÃ³n Interactiva con ML")

st.sidebar.header("Carga de Datos")
option = st.sidebar.radio("Selecciona fuente de datos:", ["ğŸ“‚ Subir CSV", "ğŸŒ Desde URL/GitHub"])

if option == "ğŸ“‚ Subir CSV":
    uploaded_file = st.sidebar.file_uploader("Sube un archivo CSV", type=["csv"])
    if uploaded_file is not None:
        df = pd.read_csv(uploaded_file)
    else:
        st.stop()
else:
    url = st.sidebar.text_input("Ingresa URL de un CSV (GitHub/raw o nube)")
    if url:
        try:
            df = pd.read_csv(url)
        except Exception as e:
            st.error(f"Error al cargar: {e}")
            st.stop()
    else:
        st.stop()

st.subheader("ğŸ“Š Vista previa de los datos")
st.dataframe(df.head())

# ===============================
# ğŸ“Œ Preprocesamiento de Datos
# ===============================
st.subheader("ğŸ§¹ Preprocesamiento de Datos")

# Eliminar duplicados
df = df.drop_duplicates()

# Rellenar valores nulos
df = df.fillna(df.mean(numeric_only=True))   # media en numÃ©ricas
df = df.fillna("desconocido")                # "desconocido" en categÃ³ricas

# DetecciÃ³n y tratamiento de atÃ­picos en columnas numÃ©ricas
num_cols = df.select_dtypes(include=np.number).columns
for col in num_cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    df[col] = np.where(df[col] < lower, lower,
              np.where(df[col] > upper, upper, df[col]))

st.write("âœ… Datos preprocesados (sin nulos, sin duplicados, atÃ­picos tratados):")
st.dataframe(df.head())

# ===============================
# ğŸ“Œ EDA rÃ¡pido
# ===============================
st.subheader("ğŸ“ˆ AnÃ¡lisis Exploratorio de Datos (EDA)")

if st.checkbox("Mostrar informaciÃ³n general"):
    st.write(df.describe())
    st.write("Valores nulos por columna:", df.isnull().sum())

if st.checkbox("DistribuciÃ³n de variables"):
    col = st.selectbox("Selecciona columna para graficar", df.columns)
    fig, ax = plt.subplots()
    sns.histplot(df[col], kde=True, ax=ax)
    st.pyplot(fig)

if st.checkbox("Matriz de correlaciÃ³n"):
    fig, ax = plt.subplots(figsize=(8,6))
    sns.heatmap(df.corr(numeric_only=True), annot=False, cmap="coolwarm", ax=ax)
    st.pyplot(fig)

# ===============================
# ğŸ“Œ SelecciÃ³n de variables
# ===============================
st.sidebar.header("ConfiguraciÃ³n del modelo")

target = st.sidebar.selectbox("Selecciona la variable objetivo (target)", df.columns)
features = st.sidebar.multiselect("Selecciona las variables predictoras", [c for c in df.columns if c != target])

if not features:
    st.warning("âš ï¸ Selecciona al menos una variable predictora")
    st.stop()

# Convertir categÃ³ricas a numÃ©ricas (OneHot Encoding)
X = pd.get_dummies(df[features], drop_first=True)

# Asegurarse de que y sea categÃ³rica
y = df[target].astype(str)

# ===============================
# ğŸ“Œ DivisiÃ³n de datos
# ===============================
test_size = st.sidebar.slider("TamaÃ±o del conjunto de prueba (%)", 10, 50, 30, step=5)

if y.nunique() < 20:
    stratify = y
else:
    stratify = None

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=test_size/100, random_state=42, stratify=stratify
)

# ===============================
# ğŸ“Œ SelecciÃ³n de modelo
# ===============================
st.sidebar.subheader("Modelo de ClasificaciÃ³n")
model_choice = st.sidebar.selectbox(
    "Â¿QuÃ© modelo quieres usar?",
    ["Ãrbol de DecisiÃ³n", "Naive Bayes", "K-Vecinos Cercanos (KNN)", "MÃ¡quina de Vectores de Soporte (SVC)", "RegresiÃ³n LogÃ­stica"]
)

# ===============================
# ğŸ“Œ HiperparÃ¡metros dinÃ¡micos
# ===============================
if model_choice == "Ãrbol de DecisiÃ³n":
    max_depth = st.sidebar.slider("Profundidad mÃ¡xima", 1, 20, 5)
    criterion = st.sidebar.selectbox("Criterio", ["gini", "entropy", "log_loss"])
    model = DecisionTreeClassifier(max_depth=max_depth, criterion=criterion, random_state=42)

elif model_choice == "Naive Bayes":
    var_smoothing = st.sidebar.slider("Var smoothing (escala log)", -12, -3, -9)
    model = GaussianNB(var_smoothing=10**var_smoothing)

elif model_choice == "K-Vecinos Cercanos (KNN)":
    n_neighbors = st.sidebar.slider("NÃºmero de vecinos (k)", 1, 20, 5)
    weights = st.sidebar.selectbox("PonderaciÃ³n", ["uniform", "distance"])
    model = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights)

elif model_choice == "MÃ¡quina de Vectores de Soporte (SVC)":
    C = st.sidebar.slider("ParÃ¡metro C", 0.01, 10.0, 1.0)
    kernel = st.sidebar.selectbox("Kernel", ["linear", "rbf", "poly", "sigmoid"])
    model = SVC(C=C, kernel=kernel)

elif model_choice == "RegresiÃ³n LogÃ­stica":
    C = st.sidebar.slider("ParÃ¡metro C (regularizaciÃ³n inversa)", 0.01, 10.0, 1.0)
    max_iter = st.sidebar.slider("Iteraciones mÃ¡ximas", 100, 1000, 300)
    model = LogisticRegression(C=C, max_iter=max_iter)

# ===============================
# ğŸ“Œ Entrenamiento
# ===============================
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# ===============================
# ğŸ“Œ Resultados
# ===============================
st.subheader("ğŸ“Œ Resultados del Modelo")

st.write("**Exactitud (Accuracy):**", accuracy_score(y_test, y_pred))
st.text("Reporte de ClasificaciÃ³n:")
st.text(classification_report(y_test, y_pred))

fig, ax = plt.subplots()
ConfusionMatrixDisplay.from_estimator(model, X_test, y_test, ax=ax, cmap="Blues")
st.pyplot(fig)

# ===============================
# ğŸ“Œ VisualizaciÃ³n del Ãrbol
# ===============================
if model_choice == "Ãrbol de DecisiÃ³n":
    st.subheader("ğŸŒ³ VisualizaciÃ³n del Ãrbol de DecisiÃ³n")
    fig, ax = plt.subplots(figsize=(12, 6))
    plot_tree(model, feature_names=X.columns, class_names=[str(c) for c in y.unique()], filled=True, ax=ax)
    st.pyplot(fig)
